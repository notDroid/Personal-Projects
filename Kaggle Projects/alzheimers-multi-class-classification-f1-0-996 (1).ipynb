{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Alzheimers Multi Class Classification","metadata":{}},{"cell_type":"markdown","source":"## Import Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport keras_cv\nfrom tensorflow.data import AUTOTUNE\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nfrom tensorflow.keras.layers import (\n    RandomBrightness, RandomZoom, RandomFlip,\n    Input, Conv2D, BatchNormalization, MaxPool2D, Dropout, Flatten, Dense\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-15T03:00:06.343673Z","iopub.execute_input":"2023-09-15T03:00:06.344092Z","iopub.status.idle":"2023-09-15T03:00:21.497383Z","shell.execute_reply.started":"2023-09-15T03:00:06.344059Z","shell.execute_reply":"2023-09-15T03:00:21.496284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data\nThis data was processed in the first part of this here: https://www.kaggle.com/code/notdroid/alzheimers-dataset-processing-outliers-imbalance/notebook","metadata":{}},{"cell_type":"code","source":"class_names = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\nimage_size = (176,208)\n\ntrain_data = tf.keras.utils.image_dataset_from_directory(\n    '/kaggle/input/alzheimers-dataset/training', \n    color_mode = 'grayscale',\n    class_names = class_names,\n    image_size = image_size,\n    label_mode = 'categorical',\n    batch_size = None\n)\n\nval_data = tf.keras.utils.image_dataset_from_directory(\n    '/kaggle/input/alzheimers-dataset/validation', \n    color_mode = 'grayscale',\n    class_names = class_names,\n    image_size = image_size,\n    label_mode = 'categorical',\n    batch_size = None\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:00:21.499611Z","iopub.execute_input":"2023-09-15T03:00:21.500394Z","iopub.status.idle":"2023-09-15T03:00:30.831224Z","shell.execute_reply.started":"2023-09-15T03:00:21.500354Z","shell.execute_reply":"2023-09-15T03:00:30.830107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add Data Augmentation\n\nWe have to be careful with our augmentation because each picture is taken in a similiar meanner, so we will do our augmentation in a minimal fashion. This makes it so that we still get the regularizing effect of sligthly varying the data in meaningful ways but not disrupting the models ability to analyze the way images were positioned.","metadata":{}},{"cell_type":"code","source":"rescaling = keras.layers.Rescaling(1./255)\n\naug = keras.Sequential([\n    Input(shape = (*image_size,1)),\n    keras_cv.layers.AutoContrast((0,1)),\n    RandomBrightness(0.15, value_range = (0,1)),\n    keras_cv.layers.RandomShear(x_factor = 0.01),\n    RandomZoom(0.01, fill_mode = 'constant'),\n    RandomFlip('horizontal')\n])","metadata":{"execution":{"iopub.status.busy":"2023-09-15T02:56:17.680537Z","iopub.execute_input":"2023-09-15T02:56:17.680896Z","iopub.status.idle":"2023-09-15T02:56:20.478150Z","shell.execute_reply.started":"2023-09-15T02:56:17.680861Z","shell.execute_reply":"2023-09-15T02:56:20.477104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Model\nHere I use a class to make tuning the model easier. The data is also converted into a tensorflow dataset with best practices for optimal results and performance used.","metadata":{}},{"cell_type":"code","source":"AUC = keras.metrics.AUC(curve = 'PR')\n\nclass disease_model:\n    def __init__(self, model = None, weight_decay = None):\n        self.model = model\n        self.weight_decay = weight_decay\n    \n    def create_dataset(self,ds,epochs,batch_size, train = True):\n        ds = ds.map(lambda image, label: (rescaling(image),label), num_parallel_calls = AUTOTUNE)\n        ds = ds.cache()\n        ds = ds.shuffle(100*batch_size)\n        ds = ds.batch(batch_size)\n        if train:\n            ds = ds.map(lambda image,label: (aug(image), label), num_parallel_calls = AUTOTUNE)\n        ds = ds.repeat(epochs)\n        ds = ds.prefetch(AUTOTUNE)\n\n        return ds\n    def conv_block(self,filters, dropout, batchnorm = True):\n        block = [\n            Conv2D(filters,3, activation = 'leaky_relu', kernel_initializer = 'he_uniform', padding = 'same'),\n            Conv2D(filters,3, activation = 'leaky_relu', kernel_initializer = 'he_uniform', padding = 'same')\n        ]\n        if batchnorm:\n            block.append(BatchNormalization())\n        block.append(MaxPool2D())\n        block.append(Dropout(dropout))\n        \n        return block\n\n    def dense_block(self,units, dropout):\n\n        return [\n            Dense(units, activation = 'leaky_relu', kernel_initializer = 'he_uniform'),\n            BatchNormalization(),\n            Dropout(dropout)\n        ]\n\n    def create_model(self, learning_rate = 1e-3):\n\n        self.model = keras.Sequential([\n            Input(shape = (*image_size,1)),\n            *self.conv_block(16, 0.5, False),\n            *self.conv_block(32, 0.2),\n            *self.conv_block(64, 0.2),\n            *self.conv_block(128, 0.2),\n            *self.conv_block(256,0.1),\n            Flatten(),\n            *self.dense_block(512, 0.5),\n            *self.dense_block(256, 0.35),\n            *self.dense_block(128, 0.2),\n            *self.dense_block(64, 0.2),\n            Dense(4)\n        ])\n        \n        opt = keras.optimizers.Adam(learning_rate, weight_decay = self.weight_decay)\n\n        self.model.compile(loss = keras.losses.CategoricalCrossentropy(from_logits = True), optimizer = 'adam', metrics = ['accuracy',AUC])\n\n        return self.model\n    \n    def train_model(self,epochs = 1, batch_size = 32, decay_step = 100, learning_rate = 1e-3):\n        if not self.model:\n            self.model = self.create_model(learning_rate)\n        checkpointer = tf.keras.callbacks.ModelCheckpoint(\n            filepath='/kaggle/working/checkpoint',\n            monitor='val_auc',\n            mode='max',\n            save_best_only=True)\n        step_decay = keras.callbacks.LearningRateScheduler(\n            lambda epoch: learning_rate*10**(-int(epoch/decay_step))\n        )\n        train_ds = self.create_dataset(train_data,epochs,batch_size)\n        val_ds = self.create_dataset(val_data,epochs,batch_size, train = False)\n        history = self.model.fit(train_ds, steps_per_epoch = 10312//batch_size,\n                  validation_data = val_ds, validation_steps = 1279//batch_size,\n                  epochs = epochs, callbacks = [step_decay,checkpointer])\n\n        return self.model, history","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:00:50.004502Z","iopub.execute_input":"2023-09-15T03:00:50.004891Z","iopub.status.idle":"2023-09-15T03:00:50.030488Z","shell.execute_reply.started":"2023-09-15T03:00:50.004857Z","shell.execute_reply":"2023-09-15T03:00:50.029196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## View Model","metadata":{}},{"cell_type":"code","source":"!pip install visualkeras\nfrom visualkeras import layered_view\n\ntest_model = disease_model().create_model()\n\ntest_model.summary()\nlayered_view(test_model, legend=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T01:19:32.483561Z","iopub.execute_input":"2023-09-15T01:19:32.484447Z","iopub.status.idle":"2023-09-15T01:19:47.141728Z","shell.execute_reply.started":"2023-09-15T01:19:32.484410Z","shell.execute_reply":"2023-09-15T01:19:47.140526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"modelling_class = disease_model(weight_decay = 1e-2)\nmodel, history = modelling_class.train_model(epochs = 150, decay_step = 50)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T01:19:47.143550Z","iopub.execute_input":"2023-09-15T01:19:47.144014Z","iopub.status.idle":"2023-09-15T02:35:33.259646Z","shell.execute_reply.started":"2023-09-15T01:19:47.143978Z","shell.execute_reply":"2023-09-15T02:35:33.258622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Model","metadata":{}},{"cell_type":"code","source":"checkpoint_path = '/kaggle/working/checkpoint'\nbest_model = keras.models.load_model(checkpoint_path)\nbest_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:00:37.295330Z","iopub.execute_input":"2023-09-15T03:00:37.295711Z","iopub.status.idle":"2023-09-15T03:00:40.445094Z","shell.execute_reply.started":"2023-09-15T03:00:37.295682Z","shell.execute_reply":"2023-09-15T03:00:40.444289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_input = Input(shape = (*image_size,1))\nx = best_model(x_input)\nx_output = keras.layers.Activation('softmax')(x)\n\nfinal_model = keras.Model(x_input,x_output)\nfinal_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy',AUC])","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:00:56.015978Z","iopub.execute_input":"2023-09-15T03:00:56.016391Z","iopub.status.idle":"2023-09-15T03:00:56.172410Z","shell.execute_reply.started":"2023-09-15T03:00:56.016358Z","shell.execute_reply":"2023-09-15T03:00:56.171406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model.save('AlzheimersModel2.h5')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T02:36:41.759888Z","iopub.execute_input":"2023-09-15T02:36:41.760307Z","iopub.status.idle":"2023-09-15T02:36:41.861645Z","shell.execute_reply.started":"2023-09-15T02:36:41.760270Z","shell.execute_reply":"2023-09-15T02:36:41.860718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Model","metadata":{}},{"cell_type":"code","source":"#10312, 1279\nX_val = np.zeros((1279,*image_size,1))\ny_val = np.zeros((1279,4))\nfor i, (x, y) in enumerate(val_data):\n    X_val[i] = x/255.\n    y_val[i] = y","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:02:53.764099Z","iopub.execute_input":"2023-09-15T03:02:53.765033Z","iopub.status.idle":"2023-09-15T03:02:57.895769Z","shell.execute_reply.started":"2023-09-15T03:02:53.764986Z","shell.execute_reply":"2023-09-15T03:02:57.894710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model):\n    y_sparse = np.argmax(y_val, axis = -1)\n    recalls = []\n    precisions = []\n    F1_scores = []\n    pred_prob = final_model.predict(X_val)\n    pred_labels = np.argmax(pred_prob, axis = -1)\n    \n    for i, class_name in enumerate(class_names):\n        T_p = np.sum((pred_labels == i)*(y_sparse == i))\n        F_p = np.sum((pred_labels == i)*(y_sparse != i))\n        F_n = np.sum((pred_labels !=i )*(y_sparse == i))\n        \n        recall = T_p/(T_p + F_n)\n        precision = T_p/(T_p + F_p)\n        F1 = 2/((1/recall)+(1/precision))\n        \n        recalls.append(recall)\n        precisions.append(precisions)\n        F1_scores.append(F1)\n        \n        print(class_name, ' recall: ', recall)\n        print(class_name, ' precision: ', precision)\n        print(class_name, ' F1: ', F1)\n        \n    macro_F1 = np.mean(F1_scores)\n    \n    print('Average F1: ', macro_F1)\n        \nevaluate_model(best_model)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:03:06.464006Z","iopub.execute_input":"2023-09-15T03:03:06.464982Z","iopub.status.idle":"2023-09-15T03:03:08.037856Z","shell.execute_reply.started":"2023-09-15T03:03:06.464914Z","shell.execute_reply":"2023-09-15T03:03:08.036628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like we can almost always be certain of our model's results, if we try evaluating on the original validation dataset we wont get the same results because of the brightness ourliers, however if we process the dataset using the same lower fence for outliers we can achieve good results.","metadata":{}}]}